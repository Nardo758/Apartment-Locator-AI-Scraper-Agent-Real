const { createClient } = require('@supabase/supabase-js');

// Production Supabase credentials
const SUPABASE_URL = 'https://jdymvpasjsdbryatscux.supabase.co';
const SUPABASE_ANON_KEY = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImpkeW12cGFzanNkYnJ5YXRzY3V4Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTg1ODc1NTgsImV4cCI6MjA3NDE2MzU1OH0.Y88-nn2LDE6qZ4p69rFuOCjM6ES027WXs-T_4g7DTso';

const supabase = createClient(SUPABASE_URL, SUPABASE_ANON_KEY);

async function triggerAtlantaScraping() {
  try {
    console.log('üîç Connected to Supabase database');

    // Get pending Atlanta jobs (limit to 100 for this test)
    const { data: jobs, error: queueError } = await supabase
      .from('scraping_queue')
      .select('id, external_id, property_id, unit_number, url, source, priority')
      .eq('status', 'pending')
      .order('priority', { ascending: false })
      .order('created_at', { ascending: true })
      .limit(100);

    if (queueError) {
      throw new Error(`Failed to fetch jobs: ${queueError.message}`);
    }

    console.log(`üìã Found ${jobs.length} pending jobs to process`);

    if (jobs.length === 0) {
      console.log('‚ùå No pending jobs found');
      return;
    }

    // Process jobs SEQUENTIALLY to avoid rate limits (not in batches)
    let processedCount = 0;
    let successCount = 0;
    let errorCount = 0;

    console.log('üöÄ Starting Atlanta property scraping (sequential processing to avoid rate limits)...');

    for (let i = 0; i < jobs.length; i++) {
      const job = jobs[i];
      console.log(`\nüì¶ Processing job ${i + 1}/${jobs.length}: ${job.url}`);

      try {
        // Update job status to processing
        const { error: processingError } = await supabase
          .from('scraping_queue')
          .update({ status: 'processing', started_at: new Date().toISOString() })
          .eq('id', job.id);

        if (processingError) {
          throw new Error(`Failed to update job status to processing: ${processingError.message}`);
        }

        // Call the Edge Function to scrape this property
        const scrapeResult = await scrapeProperty(job);

        if (scrapeResult.success) {
          // Update job status to completed
          const { error: completedError } = await supabase
            .from('scraping_queue')
            .update({ status: 'completed', completed_at: new Date().toISOString() })
            .eq('id', job.id);

          if (completedError) {
            console.error(`Failed to update job status to completed: ${completedError.message}`);
          }
          console.log(`  ‚úÖ Completed: ${job.property_id}`);
          successCount++;
        } else {
          // Update job status to failed
          const { error: failedError } = await supabase
            .from('scraping_queue')
            .update({ status: 'failed', error: scrapeResult.error })
            .eq('id', job.id);

          if (failedError) {
            console.error(`Failed to update job status to failed: ${failedError.message}`);
          }
          console.log(`  ‚ùå Failed: ${job.property_id} - ${scrapeResult.error}`);
          errorCount++;
        }

      } catch (error) {
        console.error(`  üí• Error processing ${job.property_id}:`, error.message);
        // Update job status to failed
        const { error: catchError } = await supabase
          .from('scraping_queue')
          .update({ status: 'failed', error: error.message })
          .eq('id', job.id);

        if (catchError) {
          console.error(`Failed to update job status in catch block: ${catchError.message}`);
        }
        errorCount++;
      }

      processedCount++;

      // Long delay between jobs to respect rate limits (45 seconds)
      if (i < jobs.length - 1) {
        console.log('‚è≥ Waiting 45 seconds before next job to respect Claude rate limits...');
        await new Promise(resolve => setTimeout(resolve, 45000));
      }
    }

    // Final statistics
    console.log('\nüéâ ATLANTA SCRAPING COMPLETE!');
    console.log(`üìä Total processed: ${processedCount}`);
    console.log(`‚úÖ Successful: ${successCount}`);
    console.log(`‚ùå Failed: ${errorCount}`);
    console.log(`üìà Success rate: ${((successCount / processedCount) * 100).toFixed(1)}%`);

    // Check final database state
    const jobIds = jobs.map(j => j.id);
    const { data: finalStats, error: statsError } = await supabase
      .from('scraping_queue')
      .select('status')
      .in('id', jobIds);

    if (!statsError && finalStats) {
      const statusCounts = finalStats.reduce((acc, row) => {
        acc[row.status] = (acc[row.status] || 0) + 1;
        return acc;
      }, {});

      console.log('\nüìä Final queue status:');
      Object.entries(statusCounts).forEach(([status, count]) => {
        console.log(`  ${status}: ${count}`);
      });
    }

    // Check scraped properties count
    const { count: scrapedCount, error: countError } = await supabase
      .from('apartments')
      .select('*', { count: 'exact', head: true });

    if (!countError) {
      console.log(`üè† Total properties in database: ${scrapedCount}`);
    }

  } catch (error) {
    console.error('‚ùå Error:', error);
  }
}

async function scrapeProperty(job) {
  try {
    console.log(`    üåê Multi-page scraping for: ${job.url}`);

    // Extract base URL for constructing additional page URLs
    const urlObj = new URL(job.url);
    const baseUrl = urlObj.origin;
    const basePath = urlObj.pathname.replace(/\/$/, ''); // Remove trailing slash

    // Target pages to scrape for comprehensive data (prioritized)
    const targetPages = [
      job.url, // Homepage for basic info - always first
      `${baseUrl}/floorplans/`, // Most important for pricing/unit details
      `${baseUrl}/apartments/floor-plans`, // Alternative floor plans path
      `${baseUrl}/floor-plans`, // Another common path
      `${baseUrl}/amenities/`, // Community amenities
      `${baseUrl}/features/` // Alternative amenities path
    ];

    let combinedHtml = '';
    let pagesFetched = 0;
    const maxHtmlLength = 500000; // Limit to ~500KB to avoid rate limits

    // Fetch pages in priority order, stopping if we hit size limit
    for (const pageUrl of targetPages) {
      if (combinedHtml.length > maxHtmlLength) {
        console.log(`        üìè Stopping at ${combinedHtml.length} chars to avoid rate limits`);
        break;
      }

      try {
        console.log(`      üìÑ Fetching: ${pageUrl}`);

        const response = await fetch(pageUrl, {
          headers: {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
          },
          timeout: 10000 // 10 second timeout per page
        });

        if (response.ok) {
          const pageHtml = await response.text();

          // Truncate very large pages to essential content
          let truncatedHtml = pageHtml;
          if (pageHtml.length > 100000) {
            // Keep the first 50KB and last 50KB of large pages (often headers/footers have key info)
            const firstPart = pageHtml.substring(0, 50000);
            const lastPart = pageHtml.substring(pageHtml.length - 50000);
            truncatedHtml = firstPart + '\n<!-- CONTENT TRUNCATED -->\n' + lastPart;
            console.log(`        ‚úÇÔ∏è  Truncated page from ${pageHtml.length} to ${truncatedHtml.length} chars`);
          }

          combinedHtml += `\n<!-- PAGE: ${pageUrl} -->\n${truncatedHtml}\n<!-- END PAGE: ${pageUrl} -->\n`;
          pagesFetched++;
          console.log(`        ‚úÖ Added ${truncatedHtml.length} chars from ${pageUrl.split('/').pop() || 'homepage'}`);
        } else {
          console.log(`        ‚ö†Ô∏è  Page not found (${response.status}): ${pageUrl}`);
        }
      } catch (fetchError) {
        console.log(`        ‚ö†Ô∏è  Fetch failed for ${pageUrl}: ${fetchError.message}`);
      }

      // Small delay between page requests to be respectful
      await new Promise(resolve => setTimeout(resolve, 300));
    }

    if (pagesFetched === 0) {
      throw new Error('Failed to fetch any pages from the property website');
    }

    console.log(`    üìã Combined ${pagesFetched} pages into ${combinedHtml.length} characters of HTML`);

    // Call the production Edge Function with the combined HTML
    console.log(`    ü§ñ Calling production scraper with multi-page HTML`);
    const response = await fetch('https://jdymvpasjsdbryatscux.supabase.co/functions/v1/ai-scraper-worker', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${SUPABASE_ANON_KEY}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        cleanHtml: combinedHtml,
        url: job.url,
        source: job.source,
        external_id: job.external_id,
        test_mode: true,
        multi_page: true,
        pages_fetched: pagesFetched
      })
    });

    if (!response.ok) {
      let errorMessage = `HTTP ${response.status}: ${response.statusText}`;
      try {
        const errorBody = await response.text();
        console.log(`    ‚ùå Error response body: ${errorBody}`);
        errorMessage += ` - ${errorBody}`;
      } catch (e) {
        console.log(`    ‚ùå Could not read error response body`);
      }
      throw new Error(errorMessage);
    }

    const result = await response.json();
    console.log(`    üìÑ Received response from scraper`);

    // The Edge Function should handle storing the data in the database
    // We just need to return success/failure
    return { success: true };

  } catch (error) {
    console.error(`    ‚ùå Scraper error: ${error.message}`);
    return { success: false, error: error.message };
  }
}

triggerAtlantaScraping();