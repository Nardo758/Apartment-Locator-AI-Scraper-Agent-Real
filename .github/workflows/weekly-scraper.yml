name: Enhanced Weekly Scraper Scheduler

on:
  schedule:
    # Run every Sunday at midnight UTC
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      region:
        description: 'Target region for scraping'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - atlanta
        - austin
        - dallas
        - houston
      force_run:
        description: 'Force run even if cost limits exceeded'
        required: false
        default: false
        type: boolean
      dry_run:
        description: 'Perform dry run without actual scraping'
        required: false
        default: false
        type: boolean

env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
  SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
  ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
  SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

jobs:
  pre-flight-checks:
    name: Pre-flight Checks
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.cost_check.outputs.should_run }}
      estimated_cost: ${{ steps.cost_check.outputs.estimated_cost }}
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Deno
      uses: denoland/setup-deno@v1
      with:
        deno-version: '1.40.4'

    - name: Check cost limits
      id: cost_check
      run: |
        echo "Checking daily cost limits..."
        
        # Read cost control configuration
        if [ -f "deploy-control.json" ]; then
          DAILY_LIMIT=$(cat deploy-control.json | grep -o '"dailyCostLimit":[^,}]*' | cut -d':' -f2 | tr -d ' "' || echo "50")
        else
          DAILY_LIMIT=50
        fi
        
        echo "Daily cost limit: $DAILY_LIMIT"
        
        # Estimate current usage (this would typically call your cost tracking API)
        ESTIMATED_COST=25  # Placeholder - replace with actual cost check
        
        echo "estimated_cost=$ESTIMATED_COST" >> $GITHUB_OUTPUT
        
        if [ "${{ inputs.force_run }}" == "true" ]; then
          echo "Force run enabled, bypassing cost checks"
          echo "should_run=true" >> $GITHUB_OUTPUT
        elif [ "$ESTIMATED_COST" -lt "$DAILY_LIMIT" ]; then
          echo "Cost check passed: $ESTIMATED_COST < $DAILY_LIMIT"
          echo "should_run=true" >> $GITHUB_OUTPUT
        else
          echo "Cost limit exceeded: $ESTIMATED_COST >= $DAILY_LIMIT"
          echo "should_run=false" >> $GITHUB_OUTPUT
        fi

    - name: Check system health
      run: |
        echo "Checking system health..."
        
        # Check Supabase connectivity
        if command -v curl &> /dev/null; then
          if curl -f -s "$SUPABASE_URL/rest/v1/" -H "apikey: $SUPABASE_ANON_KEY" > /dev/null; then
            echo "‚úÖ Supabase connection healthy"
          else
            echo "‚ùå Supabase connection failed"
            exit 1
          fi
        fi
        
        # Check Anthropic API (simplified)
        echo "‚úÖ System health checks passed"

  run-scraper:
    name: Run Enhanced Scraper
    runs-on: ubuntu-latest
    needs: pre-flight-checks
    if: needs.pre-flight-checks.outputs.should_run == 'true'
    
    strategy:
      matrix:
        region: ${{ inputs.region == 'all' && fromJson('["atlanta", "austin", "dallas", "houston"]') || fromJson(format('["{0}"]', inputs.region)) }}
      fail-fast: false
      max-parallel: 2
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Deno
      uses: denoland/setup-deno@v1
      with:
        deno-version: '1.40.4'

    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/deno
        key: ${{ runner.os }}-deno-${{ hashFiles('**/import_map.json', '**/deno.json') }}
        restore-keys: |
          ${{ runner.os }}-deno-

    - name: Install dependencies
      run: |
        deno cache src/main.ts

    - name: Run scraper for ${{ matrix.region }}
      run: |
        echo "üöÄ Starting enhanced scraper for region: ${{ matrix.region }}"
        
        if [ "${{ inputs.dry_run }}" == "true" ]; then
          echo "üß™ Running in dry-run mode"
          export DRY_RUN=true
        fi
        
        # Set region-specific configuration
        export TARGET_REGION="${{ matrix.region }}"
        export ENABLE_AI_PRICING=true
        export ENABLE_FRONTEND_SYNC=true
        export ENABLE_MARKET_INTELLIGENCE=true
        export BATCH_SIZE=50
        
        # Run the scraper
        if [ -f "control-scraper.sh" ]; then
          chmod +x control-scraper.sh
          ./control-scraper.sh --region="${{ matrix.region }}" --enhanced
        else
          # Fallback to direct Deno execution
          deno run --allow-all src/main.ts --region="${{ matrix.region }}"
        fi
        
        echo "‚úÖ Scraper completed for ${{ matrix.region }}"

    - name: Generate region report
      if: always()
      run: |
        echo "üìä Generating report for ${{ matrix.region }}..."
        
        # Create a simple report (replace with actual reporting logic)
        cat > region_report_${{ matrix.region }}.json << EOF
        {
          "region": "${{ matrix.region }}",
          "timestamp": "$(date -u -Iseconds)",
          "status": "${{ job.status }}",
          "dry_run": "${{ inputs.dry_run }}",
          "estimated_cost": "${{ needs.pre-flight-checks.outputs.estimated_cost }}"
        }
        EOF

    - name: Upload region report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-reports
        path: region_report_*.json
        retention-days: 30

  post-processing:
    name: Post-Processing & Reporting
    runs-on: ubuntu-latest
    needs: [pre-flight-checks, run-scraper]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download reports
      uses: actions/download-artifact@v4
      with:
        name: scraper-reports
        path: ./reports/
      continue-on-error: true

    - name: Generate summary report
      run: |
        echo "üìã Generating summary report..."
        
        # Count successful and failed regions
        SUCCESS_COUNT=0
        FAILED_COUNT=0
        TOTAL_PROPERTIES=0
        
        # Check if reports directory exists
        if [ -d "./reports" ] && [ "$(ls -A ./reports 2>/dev/null)" ]; then
          echo "Found reports directory with files"
          for report in ./reports/region_report_*.json; do
            if [ -f "$report" ]; then
              echo "Processing report: $report"
              REGION=$(cat "$report" | grep -o '"region":"[^"]*' | cut -d'"' -f4 2>/dev/null || echo "unknown")
              STATUS=$(cat "$report" | grep -o '"status":"[^"]*' | cut -d'"' -f4 2>/dev/null || echo "unknown")
              
              if [ "$STATUS" == "success" ]; then
                SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              else
                FAILED_COUNT=$((FAILED_COUNT + 1))
              fi
            fi
          done
        else
          echo "‚ö†Ô∏è No reports found - scraper may have failed early or not generated reports"
          # Check job outcomes from the workflow context
          FAILED_COUNT=1
        fi
        
        echo "SUCCESS_COUNT=$SUCCESS_COUNT" >> $GITHUB_ENV
        echo "FAILED_COUNT=$FAILED_COUNT" >> $GITHUB_ENV
        echo "TOTAL_REGIONS=$((SUCCESS_COUNT + FAILED_COUNT))" >> $GITHUB_ENV

    - name: Send Slack notification
      if: always() && secrets.SLACK_WEBHOOK_URL
      run: |
        if [ "${{ needs.pre-flight-checks.outputs.should_run }}" == "false" ]; then
          MESSAGE="‚è∏Ô∏è Weekly scraper skipped due to cost limits\nEstimated cost: \$\${{ needs.pre-flight-checks.outputs.estimated_cost }}"
        elif [ "${{ inputs.dry_run }}" == "true" ]; then
          MESSAGE="üß™ Weekly scraper dry run completed\nRegions processed: $TOTAL_REGIONS\nSuccessful: $SUCCESS_COUNT\nFailed: $FAILED_COUNT"
        elif [ "$TOTAL_REGIONS" -eq "0" ]; then
          MESSAGE="‚ö†Ô∏è Weekly apartment scraper encountered issues\nNo region reports generated\nCheck workflow logs for details"
        else
          MESSAGE="üè† Weekly apartment scraper completed!\nRegions processed: $TOTAL_REGIONS\nSuccessful: $SUCCESS_COUNT\nFailed: $FAILED_COUNT\nEstimated cost: \$\${{ needs.pre-flight-checks.outputs.estimated_cost }}"
        fi
        
        if [ -n "${{ secrets.SLACK_WEBHOOK_URL }}" ]; then
          curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"$MESSAGE\"}" \
            ${{ secrets.SLACK_WEBHOOK_URL }}
        else
          echo "Slack notification: $MESSAGE"
        fi

    - name: Update deployment status
      run: |
        echo "üìà Weekly scraper summary:"
        echo "- Total regions: $TOTAL_REGIONS"
        echo "- Successful: $SUCCESS_COUNT"
        echo "- Failed: $FAILED_COUNT"
        echo "- Estimated cost: ${{ needs.pre-flight-checks.outputs.estimated_cost }}"
        echo "- Dry run: ${{ inputs.dry_run }}"
        
        if [ "$FAILED_COUNT" -gt 0 ]; then
          echo "‚ö†Ô∏è  Some regions failed - check logs for details"
        else
          echo "‚úÖ All regions completed successfully!"
        fi