# Enhanced Weekly Scraper Scheduler with Cost Monitoring
name: Enhanced Weekly Scraper Scheduler

on:
  schedule:
    - cron: '0 0 * * 0'  # Weekly on Sunday at midnight UTC
  workflow_dispatch:      # Manual trigger
    inputs:
      force_run:
        description: 'Force run even if disabled'
        required: false
        default: 'false'
        type: boolean
      batch_size:
        description: 'Override batch size'
        required: false
        default: '50'
        type: string
      region:
        description: 'Specific region to scrape'
        required: false
        default: ''
        type: string
  repository_dispatch:    # API trigger
    types: [trigger-scraper]

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'

jobs:
  pre-flight-checks:
    runs-on: ubuntu-latest
    outputs:
      scraping_enabled: ${{ steps.check-config.outputs.enabled }}
      claude_enabled: ${{ steps.check-config.outputs.claude_enabled }}
      batch_size: ${{ steps.check-config.outputs.batch_size }}
      cost_limit: ${{ steps.check-config.outputs.cost_limit }}
      environment: ${{ steps.check-config.outputs.environment }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Install jq
      run: sudo apt-get update && sudo apt-get install -y jq
      
    - name: Check scraping configuration
      id: check-config
      run: |
        CONFIG=$(cat deploy-control.json)
        ENABLED=$(echo $CONFIG | jq -r '.scraping_enabled')
        CLAUDE_ENABLED=$(echo $CONFIG | jq -r '.claude_analysis_enabled')
        BATCH_SIZE=$(echo $CONFIG | jq -r '.batch_size')
        COST_LIMIT=$(echo $CONFIG | jq -r '.cost_limit_daily')
        ENVIRONMENT=$(echo $CONFIG | jq -r '.environment')
        
        echo "enabled=$ENABLED" >> $GITHUB_OUTPUT
        echo "claude_enabled=$CLAUDE_ENABLED" >> $GITHUB_OUTPUT
        echo "batch_size=$BATCH_SIZE" >> $GITHUB_OUTPUT
        echo "cost_limit=$COST_LIMIT" >> $GITHUB_OUTPUT
        echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
        
        echo "Configuration loaded:"
        echo "  Scraping Enabled: $ENABLED"
        echo "  Claude Enabled: $CLAUDE_ENABLED"
        echo "  Batch Size: $BATCH_SIZE"
        echo "  Daily Cost Limit: \$$COST_LIMIT"
        echo "  Environment: $ENVIRONMENT"
        
    - name: Check daily cost limit
      id: cost-check
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      run: |
        # Query current daily costs from database
        DAILY_COST=$(curl -s -X POST \
          "$SUPABASE_URL/rest/v1/rpc/get_daily_scraping_cost" \
          -H "Authorization: Bearer $SUPABASE_SERVICE_ROLE_KEY" \
          -H "Content-Type: application/json" \
          -H "Prefer: return=representation" \
          -d '{}' | jq -r '.cost // 0')
        
        echo "current_daily_cost=$DAILY_COST" >> $GITHUB_OUTPUT
        echo "Current daily cost: \$$DAILY_COST"
        
        COST_LIMIT=${{ steps.check-config.outputs.cost_limit }}
        if (( $(echo "$DAILY_COST >= $COST_LIMIT" | bc -l) )); then
          echo "‚ö†Ô∏è Daily cost limit reached: \$$DAILY_COST >= \$$COST_LIMIT"
          echo "cost_limit_reached=true" >> $GITHUB_OUTPUT
        else
          echo "‚úÖ Within daily cost limit: \$$DAILY_COST < \$$COST_LIMIT"
          echo "cost_limit_reached=false" >> $GITHUB_OUTPUT
        fi

  scrape-properties:
    needs: pre-flight-checks
    runs-on: ubuntu-latest
    if: |
      (needs.pre-flight-checks.outputs.scraping_enabled == 'true' || github.event.inputs.force_run == 'true') &&
      needs.pre-flight-checks.outputs.cost_limit_reached != 'true'
    
    strategy:
      matrix:
        region: [atlanta, new-york, chicago, miami, dallas]
      fail-fast: false
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        
    - name: Install dependencies
      run: |
        npm install -g cron-parser
        
    - name: Run Regional Scraping
      id: scrape
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      run: |
        REGION="${{ matrix.region }}"
        BATCH_SIZE="${{ github.event.inputs.batch_size || needs.pre-flight-checks.outputs.batch_size }}"
        SPECIFIC_REGION="${{ github.event.inputs.region }}"
        
        # Skip if specific region requested and this isn't it
        if [[ -n "$SPECIFIC_REGION" && "$REGION" != "$SPECIFIC_REGION" ]]; then
          echo "Skipping $REGION (specific region: $SPECIFIC_REGION)"
          exit 0
        fi
        
        echo "üåç Processing region: $REGION with batch size: $BATCH_SIZE"
        
        # Call scheduled scraper function with region parameter
        RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
          "$SUPABASE_URL/functions/v1/scheduled-scraper" \
          -H "Authorization: Bearer $SUPABASE_ANON_KEY" \
          -H "Content-Type: application/json" \
          -d "{
            \"region\": \"$REGION\",
            \"batch_size\": $BATCH_SIZE,
            \"claude_enabled\": ${{ needs.pre-flight-checks.outputs.claude_enabled }},
            \"source\": \"github_actions\"
          }")
        
        HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
        BODY=$(echo "$RESPONSE" | head -n -1)
        
        echo "Response Code: $HTTP_CODE"
        echo "Response Body: $BODY"
        
        if [[ "$HTTP_CODE" == "200" ]]; then
          echo "‚úÖ Successfully processed $REGION"
          
          # Extract metrics from response
          PROCESSED=$(echo "$BODY" | jq -r '.processed // 0')
          COST=$(echo "$BODY" | jq -r '.cost // 0')
          
          echo "processed=$PROCESSED" >> $GITHUB_OUTPUT
          echo "cost=$COST" >> $GITHUB_OUTPUT
          echo "region=$REGION" >> $GITHUB_OUTPUT
        else
          echo "‚ùå Failed to process $REGION"
          exit 1
        fi
        
    - name: Update cost tracking
      if: steps.scrape.outputs.processed > 0
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      run: |
        # Record the scraping operation cost
        curl -s -X POST \
          "$SUPABASE_URL/rest/v1/rpc/rpc_inc_scraping_costs" \
          -H "Authorization: Bearer $SUPABASE_SERVICE_ROLE_KEY" \
          -H "Content-Type: application/json" \
          -d "{
            \"operation_type\": \"github_actions_scraping\",
            \"cost_amount\": ${{ steps.scrape.outputs.cost }},
            \"metadata\": {
              \"region\": \"${{ steps.scrape.outputs.region }}\",
              \"properties_processed\": ${{ steps.scrape.outputs.processed }},
              \"workflow_run\": \"${{ github.run_id }}\"
            }
          }"

  claude-intelligence:
    needs: [pre-flight-checks, scrape-properties]
    runs-on: ubuntu-latest
    if: |
      needs.pre-flight-checks.outputs.claude_enabled == 'true' &&
      !failure() && !cancelled()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Run Claude Property Intelligence
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      run: |
        echo "üß† Running Claude property intelligence analysis"
        
        RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
          "$SUPABASE_URL/functions/v1/property-researcher" \
          -H "Authorization: Bearer $SUPABASE_ANON_KEY" \
          -H "Content-Type: application/json" \
          -d "{
            \"mode\": \"batch_analysis\",
            \"batch_size\": 20,
            \"source\": \"github_actions\"
          }")
        
        HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
        BODY=$(echo "$RESPONSE" | head -n -1)
        
        if [[ "$HTTP_CODE" == "200" ]]; then
          echo "‚úÖ Claude intelligence analysis completed"
          echo "$BODY" | jq '.'
        else
          echo "‚ö†Ô∏è Claude intelligence analysis had issues (non-critical)"
          echo "Response: $BODY"
        fi

  post-run-analysis:
    needs: [pre-flight-checks, scrape-properties, claude-intelligence]
    runs-on: ubuntu-latest
    if: always() && !cancelled()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Generate run summary
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      run: |
        echo "üìä Generating weekly scraper run summary"
        
        # Get today's scraping statistics
        STATS=$(curl -s -X POST \
          "$SUPABASE_URL/rest/v1/rpc/get_daily_scraping_stats" \
          -H "Authorization: Bearer $SUPABASE_SERVICE_ROLE_KEY" \
          -H "Content-Type: application/json" \
          -d '{}')
        
        echo "Today's Scraping Statistics:"
        echo "$STATS" | jq '.'
        
        TOTAL_COST=$(echo "$STATS" | jq -r '.total_cost // 0')
        TOTAL_PROPERTIES=$(echo "$STATS" | jq -r '.total_properties // 0')
        SUCCESS_RATE=$(echo "$STATS" | jq -r '.success_rate // 0')
        
        echo "Summary:"
        echo "- Total Properties Processed: $TOTAL_PROPERTIES"
        echo "- Total Cost: \$$TOTAL_COST"
        echo "- Success Rate: $SUCCESS_RATE%"
        
        # Check if we need to send alerts
        COST_LIMIT=${{ needs.pre-flight-checks.outputs.cost_limit }}
        if (( $(echo "$TOTAL_COST >= $COST_LIMIT * 0.8" | bc -l) )); then
          echo "‚ö†Ô∏è Cost approaching daily limit (80% of \$$COST_LIMIT)"
        fi
        
    - name: Send Slack notification
      if: env.SLACK_WEBHOOK_URL != ''
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      run: |
        curl -X POST -H 'Content-type: application/json' \
          --data "{
            \"text\": \"üè† Weekly Property Scraper Complete\",
            \"blocks\": [
              {
                \"type\": \"section\",
                \"text\": {
                  \"type\": \"mrkdwn\",
                  \"text\": \"*Weekly Property Scraper Results*\n‚Ä¢ Workflow: ${{ github.workflow }}\n‚Ä¢ Status: ${{ job.status }}\n‚Ä¢ Run ID: ${{ github.run_id }}\"
                }
              }
            ]
          }" \
          $SLACK_WEBHOOK_URL

  skip-scraping:
    needs: pre-flight-checks
    runs-on: ubuntu-latest
    if: |
      needs.pre-flight-checks.outputs.scraping_enabled != 'true' &&
      github.event.inputs.force_run != 'true'
    
    steps:
    - name: Skip notification
      run: |
        echo "üö´ Scraping is disabled in configuration"
        echo "Current status:"
        echo "  Scraping Enabled: ${{ needs.pre-flight-checks.outputs.scraping_enabled }}"
        echo "  Environment: ${{ needs.pre-flight-checks.outputs.environment }}"
        echo ""
        echo "To enable scraping:"
        echo "  1. Update deploy-control.json: scraping_enabled = true"
        echo "  2. Commit and push changes"
        echo "  3. Or run manually with force_run = true"

  cost-limit-reached:
    needs: pre-flight-checks
    runs-on: ubuntu-latest
    if: needs.pre-flight-checks.outputs.cost_limit_reached == 'true'
    
    steps:
    - name: Cost limit notification
      run: |
        echo "üí∞ Daily cost limit has been reached"
        echo "Current daily cost limit: \$${{ needs.pre-flight-checks.outputs.cost_limit }}"
        echo "Skipping scraping to prevent overspend"
        echo ""
        echo "To increase limit, update deploy-control.json and redeploy"